{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1I7Yg_29YVPDBjCuHECDe9Lfh9DcGTcYq","timestamp":1695241151441}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Goal:\n","Implement linear and logistic regression, using the gradient descent algorithm.\n","\n","Data:\n","Synthetic data.\n","\n","- X is a matrix of shape (100, 4) where the first column is filled with ones, and the remaining three columns contain random values sampled from a normal distribution with mean 0 and standard deviation 3.\n","\n","- Y is a target vector of shape (100,) calculated as a linear combination of the first two columns of X multiplied by coefficients [1.5, 2.5], with some added random noise.\n","\n","- Y2 is generated as a binary label (0 or 1) indicating whether Y is greater than 9. It's calculated using the astype method to convert True and False values from a boolean condition into 1 and 0, respectively.\n","\n","Function `get_gradient`:\n","\n","This function computes the gradient of a loss with respect to the model parameters (betas) using matrix operations. It calculates the dot product of the transposed feature matrix X.T and the loss vector loss, and then divides by the number of rows in X to get the average gradient."],"metadata":{"id":"GaDuoH1O3zfH"}},{"cell_type":"markdown","source":["This question has three parts:\n","\n","1. Write a `gradient_descent` function that returns the optimal betas to minimize mean square error (the standard L2 error) predictions against the target variable, Y, for linear regression.\n","2. Generalize the above function to work for classification tasks as well as regression tasks (i.e., allow it to work for logistic regression).\n","3. Write two functions, `find_mean_absolute_in_sample_error` and `find_in_sample_accuracy` which evaluate the in-sample efficacy of the `gradient_descent` function for (1) and (2) using the standard definitions of (classification) accuracy and mean absolute (regression) error."],"metadata":{"id":"uCHO3kdQ5Z1W"}},{"cell_type":"markdown","source":["1. Define the `gradient_descent` function: This function performs gradient descent to optimize the model parameters (betas) for linear regression. It takes several parameters:\n","\n","  - `X`: The feature matrix.\n","  - `Y`: The target vector.\n","  - `lr`: The learning rate for gradient descent.\n","  - `iterations`: The number of gradient descent iterations.\n","  - `fn`: An optional function to apply to the predicted values (hypothesis).\n","\n","  It initializes betas with ones. It iterates for the specified number of iterations, calling the `update_step` function to update the betas in each iteration.\n","\n","  `update_step` function updates the model parameters (betas) in each iteration of gradient descent.\n","  - It computes the hypothesis by applying the function fn to the dot product of X and betas.\n","  - It calculates the loss as the difference between the hypothesis and the actual target Y.\n","  - It computes the gradient of the loss using the get_gradient function.\n","  - It updates the betas by subtracting the gradient multiplied by the learning rate lr.\n"],"metadata":{"id":"UJZFIJQB6Ldd"}},{"cell_type":"code","source":["# Part 1 - gradient descent function\n","\n","import numpy as np\n","\n","np.random.seed(222)\n","\n","X = np.c_[np.ones(100), np.random.randn(100, 3) * 3]\n","Y = np.sum(X[:, :2] * np.array([1.5, 2.5]), axis=1) + np.random.randn(100) / 10\n","Y2 = (Y > 9).astype(int)\n","\n","# Initiate betas to test in functions:\n","betas = np.array([1,1,1,1])\n","\n","# Function to get predicted Y\n","def get_pred(X, betas):\n","  Y_predicted = np.dot(X, betas)\n","  return Y_predicted\n","\n","Y_predicted = get_pred(X, betas)\n","print(f\"Y_Predicted: {Y_predicted[:10]}\")\n","print(\"\")\n","\n","# Function to get loss/cost/error\n","def get_loss(Y_predicted, Y):\n","  error = Y_predicted - Y\n","  return error\n","\n","loss = get_loss(Y_predicted, Y)\n","print(f\"loss: {loss[:10]}\")\n","print(\"\")\n","\n","# Function to get gradient\n","def get_gradient(X, loss):\n","    return np.dot(X.T, loss) / X.size\n","\n","gradient = get_gradient(X, loss)\n","print(f\"gradient: {gradient}\")\n","print(\"\")\n","\n","# Function to do the work of adjusting betas for each iteration\n","def update_step(X, Y, lr, betas, fn):\n","  Y_predicted = get_pred(X, betas)\n","  loss = get_loss(Y_predicted, Y)\n","  gradient = get_gradient(X, loss)\n","  betas = betas - lr * gradient\n","  return betas\n","\n","# Function to use gradient descent to find optimal (or final) betas\n","def gradient_descent(X, Y, lr, iterations, fn):\n","  betas = np.array([1,1,1,1])\n","  for i in range(iterations):\n","    betas = update_step(X, Y, lr, betas, fn)\n","  return betas\n","\n","# Call the gradient_descent function\n","betas = gradient_descent(X=X, Y=Y, lr=0.01, iterations=int(1e5), fn=get_pred)\n","\n","# Print the optimized betas (model parameters)\n","print(\"Optimized Betas:\", betas)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVCGAKPv6O8t","executionInfo":{"status":"ok","timestamp":1695283151718,"user_tz":360,"elapsed":779,"user":{"displayName":"Joe Hutchings","userId":"08051434744775910608"}},"outputId":"e7691497-b66d-4d24-9fdd-3211a84d3b78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Y_Predicted: [ 9.09355874  3.87807104 -0.27301594  1.77942157 -0.49891288  2.45864619\n"," -6.47319807  0.51366967  3.62338308  2.20001222]\n","\n","loss: [-7.22686574 -5.13144378  5.11621034 -1.98458124 -8.79627048  5.54257905\n"," -6.26572512  4.59660213 -1.04582718 -5.504468  ]\n","\n","gradient: [-0.17515679 -3.42504589  2.7737223   1.84624724]\n","\n","Optimized Betas: [ 1.49760412e+00  2.50064953e+00 -1.62079328e-03  4.03070779e-03]\n"]}]},{"cell_type":"code","source":["# Part 2 - Generalize the gradient descent function to accomodate logistic regression (classification)\n","import numpy as np\n","\n","np.random.seed(333)\n","\n","X = np.c_[np.ones(100), np.random.randn(100, 3) * 3]\n","Y = np.sum(X[:, :2] * np.array([1.5, 2.5]), axis=1) + np.random.randn(100) / 10\n","Y2 = (Y > 9).astype(int)\n","\n","# Initiate betas to test in functions:\n","betas = np.array([1,1,1,1])\n","\n","# Function to get predicted Y\n","def get_pred(X, betas, type):\n","  if type == 'linear':\n","    Y_predicted = np.dot(X, betas)\n","  if type == 'logistic':\n","    Y_predicted = 1/(1 + np.exp(-np.dot(X, betas)))\n","  return Y_predicted\n","\n","Y_predicted = get_pred(X, betas, 'linear')\n","print(f\"Y_Predicted: {Y_predicted[:10]}\")\n","print(\"\")\n","\n","Y2_predicted = get_pred(X, betas, 'logistic')\n","print(f\"Y2_Predicted: {Y2_predicted[:10]}\")\n","print(\"\")\n","\n","# Function to get loss/cost/error\n","def get_loss(Y_predicted, Y):\n","  error = Y_predicted - Y\n","  return error\n","\n","loss = get_loss(Y_predicted, Y)\n","print(f\"loss: {loss[:10]}\")\n","print(\"\")\n","\n","loss_logistic = get_loss(Y2, Y2_predicted)\n","print(f\"loss_logistic: {loss_logistic[:10]}\")\n","print(\"\")\n","\n","# Function to get gradient\n","def get_gradient(X, loss):\n","    return np.dot(X.T, loss) / X.size\n","\n","gradient = get_gradient(X, loss)\n","print(f\"gradient: {gradient}\")\n","print(\"\")\n","\n","# Function to do the work of adjusting betas for each iteration\n","def update_step(X, Y, lr, betas, fn, type):\n","  Y_predicted = get_pred(X, betas, type)\n","  loss = get_loss(Y_predicted, Y)\n","  gradient = get_gradient(X, loss)\n","  betas = betas - lr * gradient\n","  return betas\n","\n","# Function to use gradient descent to find optimal (or final) betas\n","def gradient_descent(X, Y, lr, iterations, fn, type):\n","  betas = np.array([1,1,1,1])\n","  for i in range(iterations):\n","    betas = update_step(X, Y, lr, betas, fn, type)\n","  return betas\n","\n","# Call the gradient_descent function for linear regression\n","betas = gradient_descent(X=X, Y=Y, lr=0.01, iterations=int(1e5), fn=get_pred, type='linear')\n","\n","# Print the optimized betas (model parameters)\n","print(\"Optimized Betas:\", betas)\n","\n","# Call the gradient_descent function for logistic regression\n","betas_logistic = gradient_descent(X, Y2, 0.01, int(1e5), get_pred, 'logistic')\n","\n","# Print the optimized betas (model parameters)\n","print(\"Optimized Betas Logistic:\", betas_logistic)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88EBdlZ4Ym1G","executionInfo":{"status":"ok","timestamp":1695283158009,"user_tz":360,"elapsed":2027,"user":{"displayName":"Joe Hutchings","userId":"08051434744775910608"}},"outputId":"9ba2259f-ec44-4367-ffaf-a96d73268a9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Y_Predicted: [ 5.70200267  1.0875563  -4.06201709 -0.73086338  6.76813561 -1.9990152\n","  1.65343071 -6.04055793  8.23248756 -0.34567919]\n","\n","Y2_Predicted: [0.99667184 0.74792128 0.01692295 0.32500529 0.99885148 0.11930636\n"," 0.83935418 0.00237458 0.9997342  0.4144306 ]\n","\n","loss: [ -8.6178924    0.54899833  -0.0984822    6.90611579   6.27698241\n","  -2.06954166 -11.9855752   -5.64039573   4.42050211  -3.22940331]\n","\n","loss_logistic: [ 0.00332816 -0.74792128 -0.01692295 -0.32500529 -0.99885148 -0.11930636\n","  0.16064582 -0.00237458 -0.9997342  -0.4144306 ]\n","\n","gradient: [-0.25304198 -3.17408828  1.61390671  2.06269753]\n","\n","Optimized Betas: [1.49247225e+00 2.49818358e+00 1.73030386e-03 4.89803525e-04]\n","Optimized Betas Logistic: [-6.13027773  2.10620126  0.0794197   0.07156676]\n"]}]},{"cell_type":"markdown","source":["2. Create a `find_mean_absolute_in_sample_error` function designed to calculate the Mean Absolute In-Sample Error (MAISE) for a linear regression model.\n","\n","  1.  Function Definition: The function is defined with three parameters: X, Y, and betas.\n","  * X: The feature matrix.\n","  * Y: The target vector.\n","  * betas: The model parameters obtained from the gradient_descent function. It is set to the result of gradient_descent() by default.\n","\n","  2.  Calculation of Predictions: It calculates the predicted values (hypotheses) for the given feature matrix X and the model parameters betas.\n","\n","  3.  Calculation of Absolute Errors: The next step calculates the absolute errors by subtracting the actual target values Y from the predicted values obtained in the previous step\n","\n","  4.  Calculation of Mean Absolute Error (MAE): The MAE represents the average absolute difference between the predicted and actual values. It is a measure of how well the linear regression model fits the data.\n"],"metadata":{"id":"iHNPt_jK8u0S"}},{"cell_type":"code","source":["def find_mean_absolute_in_sample_error(X=X, Y=Y, betas=gradient_descent):\n","  Y_prediction = get_pred(X, betas, 'linear')\n","  error = Y_prediction - Y\n","  abs_error = np.abs(error)\n","  mae = np.mean(abs_error)\n","  return mae\n","  \"\"\"Should be around 0.08.\"\"\""],"metadata":{"id":"PWtiSK698uI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the find_mean_absolute_in_sample_error function\n","maise = find_mean_absolute_in_sample_error(X=X, Y=Y, betas=gradient_descent(X=X, Y=Y, lr=0.001, iterations=int(1e5), fn=get_pred, type='linear'))\n","\n","# Print the Mean Absolute In-Sample Error (MAISE)\n","print(\"Mean Absolute In-Sample Error (MAISE):\", maise)"],"metadata":{"id":"AS_vuRc79aF0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695283164834,"user_tz":360,"elapsed":760,"user":{"displayName":"Joe Hutchings","userId":"08051434744775910608"}},"outputId":"481c058f-6102-4cc9-c014-ac25565d904f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute In-Sample Error (MAISE): 0.06745953846225966\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"SrGQrlib9OBe"}}]}